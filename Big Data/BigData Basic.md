Big Data Basic
==============
기존 데이터베이스 관리도구의 능력을 넘어서 대량(수십테라바이트)의 정형 또는 테이터베이스 형태가 아닌 비정형의 데이터 집합까지 포함한 데이터로부터 가치를 추출하고 결과를 분석하는 기술이다.


## 정의
빅 데이터는 통상적으로 사용되는 데이터 수집, 관리 및 처리 소프트웨어의 수용 한계를 넘어서는 크기의 데이터를 말한다. 빅 데이터의 사이즈는 단일 데이터 집합의 크기가 끊임없이 변화하는 것이 특징이다. 또한 카트너(미국의 정보 기술 연구 및 자문 회사)의 애널리스트 더그 레이니가 빅데이터의 3요소를 정의 하였다. 빅 데이터를 이해하기 위해 3v의 이해가 필요하다.


## 빅데이터의 3대 요소(3v)
빅 데이터의 3대 요소(3v)란 크기(Volume), 속도(Velocity), 다양성(Variety)를 의미한다.
![http://cfile23.uf.tistory.com/image/1618114950017CF83B8DD1](./Image/3v.jpg)


### 크기(Volume)
__데이터의 양__
비즈니스 특성에 따라 다를 수 있지만, 일반적으로 수십 테라 혹은 수십 페타 바이트 이상의 빅 데이터의 범위에 해당한다. 이러한 빅 데이터는 기존 파일 시스템에 저장하기 어렵다. 또한 데이터 분석을 위해서 사용하고 BI/DW 같은 솔루션에서 해결하기 어려울 정도로 급격하게 데이터 양이 증가하고 있다. 이러한 문제를 해결/극복하려면 확장 가능한 방식으로 데이터를 저장하고 분석하는 분산 컴퓨팅 방식으로 접근해야 한다. 현재 분산 컴퓨팅 솔루션에는 구글의 GFS, 아파치의 하둡 등이 있다. 대용량 병렬 처리 데이터베이스로는 EMC의 GreenPlum, HP의 Vertica, IBM의 Netezza, 테라데이터의 Kickfire 등이 있다.
